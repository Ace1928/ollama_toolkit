.. Ollama Toolkit documentation master file

# Ollama Toolkit Documentation

Welcome to the Ollama Toolkit documentationâ€”where precision meets possibility. This Python client provides a comprehensive interface to interact with Ollama, a framework for running large language models locally with **exceptional efficiency**.

## Overview

Ollama Toolkit gives you programmatic access to:

- Text generation with surgical precision
- Chat completion engineered for natural flow
- Embedding creation with mathematical elegance
- Model management (listing, pulling, copying, deleting) with controlled recursion
- Async operations for high-velocity performance applications
- Robust error handling with intuitive fallback mechanisms
- Automatic Ollama installation and startup with zero friction

## Key Features

- ğŸš€ **Complete API Coverage**: Support for all Ollama endpointsâ€”nothing missing, nothing extra
- ğŸ”„ **Recursive Async Support**: Both synchronous and asynchronous interfaces that build upon each other
- ğŸ”§ **Structurally Perfect CLI**: Powerful command-line tools with intuitive architecture
- ğŸ”Œ **Zero-Friction Auto-Installation**: Install and start Ollama with mathematically minimal steps
- ğŸ’ª **Self-Aware Error Handling**: Comprehensive error types that explain precisely what went wrong
- ğŸ“Š **Velocity-Optimized Embeddings**: Create and manipulate embeddings with maximum efficiency
- ğŸ§ª **Recursively Refined Testing**: Every function proven robust through iterative improvement

## Getting Started

```python
# This implementation follows Eidosian principles of contextual integrity and precision
from ollama_toolkit import OllamaClient
from ollama_toolkit.utils.common import ensure_ollama_running

# Ensure Ollama is installed and running - structurally sound foundation
is_running, message = ensure_ollama_running()
if not is_running:
    print(f"Error: {message}")
    exit(1)
    
print(f"Ollama status: {message}")

# Initialize the client with optimal timeout
client = OllamaClient(timeout=300)  # 5-minute timeout for larger operations

# Check version - self-aware system verification
version = client.get_version()
print(f"Connected to Ollama version: {version['version']}")

# List available models - structural awareness
models = client.list_models()
model_names = [model["name"] for model in models.get("models", [])]
print(f"Available models: {model_names}")

# Generate text with precision and flow
if model_names:  # Use first available model if any exist
    model_name = model_names[0]
    print(f"\nGenerating text with {model_name}...")
    
    response = client.generate(
        model=model_name,
        prompt="Explain what makes a good API in three sentences.",
        stream=False
    )
    
    print(f"\nResponse: {response.get('response', 'No response generated')}")
else:
    print("No models available. Use client.pull_model() to download a model.")
```

## Installation Guide

- Install via PyPI: `pip install ollama-toolkit`
- Or install locally in editable mode:

```bash
pip install -e /path/to/ollama_toolkit
```

## Documentation Contents

```{toctree}
:maxdepth: 2
:caption: ğŸ“š Getting Started

README
installation
quickstart
```

```{toctree}
:maxdepth: 2
:caption: ğŸ› ï¸ Core Documentation

api_reference
examples
advanced_usage
```

```{toctree}
:maxdepth: 2
:caption: ğŸ”„ Features & Capabilities

chat
generate
embed
```

```{toctree}
:maxdepth: 2
:caption: ğŸ§  Guides & References

conventions
troubleshooting
eidosian_integration
version
contributing
```

## Examples

Check out these practical examples:

- [Basic Usage](examples.md) â€” Precision in simplicity  
- [Generate Text](generate.md) â€” Flow like a river  
- [Chat Completion](chat.md) â€” Universal yet personal  
- [Embeddings](embed.md) â€” Mathematical elegance

All examples can be run via:

```bash
python -m ollama_toolkit.examples.<example_file>
```
(e.g. `python -m ollama_toolkit.examples.quickstart`).

## License

This project is licensed under the MIT License - see the LICENSE file for details.
